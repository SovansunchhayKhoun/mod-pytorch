{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# http://127.0.0.1:8888/tree?token=2449353bcd2c11de2c171e95996e620551b97049da6c4487"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(random.random())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device agnostic code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cpu'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating data for testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\runpy.py\", line 196, in _run_module_as_main\n",
      "    return _run_code(code, main_globals, None,\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\runpy.py\", line 86, in _run_code\n",
      "    exec(code, run_globals)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel_launcher.py\", line 18, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\traitlets\\config\\application.py\", line 1075, in launch_instance\n",
      "    app.start()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\kernelapp.py\", line 739, in start\n",
      "    self.io_loop.start()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\tornado\\platform\\asyncio.py\", line 205, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\asyncio\\base_events.py\", line 603, in run_forever\n",
      "    self._run_once()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\asyncio\\base_events.py\", line 1909, in _run_once\n",
      "    handle._run()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\asyncio\\events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 545, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 534, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 437, in dispatch_shell\n",
      "    await result\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 362, in execute_request\n",
      "    await super().execute_request(stream, ident, parent)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\kernelbase.py\", line 778, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\ipkernel.py\", line 449, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\ipykernel\\zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3075, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3130, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 128, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3334, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3517, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"d:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3577, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"C:\\Users\\sunch\\AppData\\Local\\Temp\\ipykernel_4768\\51030591.py\", line 8, in <module>\n",
      "    X = torch.arange(start, end, step)\n",
      "C:\\Users\\sunch\\AppData\\Local\\Temp\\ipykernel_4768\\51030591.py:8: UserWarning: Failed to initialize NumPy: _ARRAY_API not found (Triggered internally at ..\\torch\\csrc\\utils\\tensor_numpy.cpp:84.)\n",
      "  X = torch.arange(start, end, step)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(500000, 500000)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = 0.7\n",
    "bias = 0.3\n",
    "\n",
    "start = 0\n",
    "end = 1000\n",
    "step = 0.002\n",
    "\n",
    "X = torch.arange(start, end, step)\n",
    "y = weight * X + bias\n",
    "\n",
    "y = y.unsqueeze(dim=1)\n",
    "\n",
    "len(X), len(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([500000]), torch.Size([500000, 1]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "INPUT_FEATURE = 500000\n",
    "HIDDEN_UNIT = 4\n",
    "OUTPUT_FEATURE = 1\n",
    "class CustomModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear_layer_1 = nn.Linear(in_features=INPUT_FEATURE, out_features=HIDDEN_UNIT)\n",
    "        self.linear_layer_2 = nn.Linear(in_features=HIDDEN_UNIT, out_features=HIDDEN_UNIT)\n",
    "        self.linear_layer_3 = nn.Linear(in_features=HIDDEN_UNIT, out_features=HIDDEN_UNIT)\n",
    "        self.linear_layer_4 = nn.Linear(in_features=HIDDEN_UNIT, out_features=HIDDEN_UNIT)\n",
    "        self.linear_layer_5 = nn.Linear(in_features=HIDDEN_UNIT, out_features=OUTPUT_FEATURE)\n",
    "    def forward(self, x):\n",
    "        x = self.linear_layer_1(x)\n",
    "        print(\"ðŸš€ ~ x = self.linear_layer_1(x):\", x.device, x[:5])\n",
    "        print(f\"-------------------------------\")\n",
    "        x = self.linear_layer_2(x)\n",
    "        print(\"ðŸš€ ~ x = self.linear_layer_2(x):\", x.device, x[:5])\n",
    "        print(f\"-------------------------------\")\n",
    "        x = self.linear_layer_3(x)\n",
    "        print(\"ðŸš€ ~ x = self.linear_layer_3(x):\", x.device, x[:5])\n",
    "        print(f\"-------------------------------\")\n",
    "        x = self.linear_layer_4(x)\n",
    "        print(\"ðŸš€ ~ x = self.linear_layer_4(x):\", x.device, x[:5])\n",
    "        print(f\"-------------------------------\")\n",
    "        x = self.linear_layer_5(x)\n",
    "        print(\"ðŸš€ ~ x = self.linear_layer_5(x):\", x.device, x[:5])\n",
    "        print(f\"-------------------------------\")\n",
    "        return x\n",
    "        # return self.linear_layer_5(self.linear_layer_4(self.linear_layer_3(self.linear_layer_2(self.linear_layer_1(x)))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0 = CustomModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_0.set_layer_resource([0, 0, 0, 0, 0])\n",
    "model_0.set_layer_resource([1, 1, 1, 1, 1])\n",
    "# model_0.set_layer_resource([1, 0, 0, 0, 0])\n",
    "# model_0.set_layer_resource([1, 1, 0, 0, 0])\n",
    "# model_0.set_layer_resource([1, 1, 1, 0, 0])\n",
    "# model_0.set_layer_resource([1, 1, 1, 1, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_0.set_layer_resource([1, 1, 1, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ ------------- ~ _call_impl:\n",
      "Try block\n",
      "ðŸš€ ~ _global_forward_pre_hooks or self._forward_pre_hooks::\n",
      "_call_impl else Args: tensor([0.0000, 0.0020, 0.0040, 0.0060, 0.0080]):\n",
      "Forward hook called (internal)\n",
      "Resource allocated to defined layer resource: [1, 1, 1, 1, 1]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Cannot allocate Linear(in_features=500000, out_features=4, bias=True) to cuda",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m model_0\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39minference_mode():\n\u001b[1;32m----> 6\u001b[0m   y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel_0\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m end_time \u001b[38;5;241m=\u001b[39m timer()\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTotal time: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mend_time\u001b[38;5;250m \u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;250m \u001b[39mstart_time\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32md:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1591\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1589\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1590\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1591\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32md:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1636\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1634\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1635\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_call_impl else Args: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00margs[\u001b[38;5;241m0\u001b[39m][:\u001b[38;5;241m5\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1636\u001b[0m     args_result \u001b[38;5;241m=\u001b[39m \u001b[43mhook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1637\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m args_result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1638\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(args_result, \u001b[38;5;28mtuple\u001b[39m):\n",
      "File \u001b[1;32md:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:1571\u001b[0m, in \u001b[0;36mModule.forward_pre_hook\u001b[1;34m(self, input, output)\u001b[0m\n\u001b[0;32m   1569\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28minput\u001b[39m, torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39m_Loss):\n\u001b[0;32m   1570\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mForward hook called (internal)\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m-> 1571\u001b[0m     layer_resource \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregister_allocate_layer_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1572\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer resource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_resource\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   1574\u001b[0m     layer_device \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mparameters())\u001b[38;5;241m.\u001b[39mdevice\n",
      "File \u001b[1;32md:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:507\u001b[0m, in \u001b[0;36mModule.register_allocate_layer_resource\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    505\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    506\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mResource allocated to defined layer resource: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_layer_resource()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 507\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mallocate_layer_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_layer_resource\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    508\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_layer_resource()\n",
      "File \u001b[1;32md:\\Dgist Intern\\mod-pytorch\\.conda\\lib\\site-packages\\torch\\nn\\modules\\module.py:491\u001b[0m, in \u001b[0;36mModule.allocate_layer_resource\u001b[1;34m(self, config)\u001b[0m\n\u001b[0;32m    489\u001b[0m         module \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mcuda()\n\u001b[0;32m    490\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 491\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCannot allocate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to cuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    493\u001b[0m     module \u001b[38;5;241m=\u001b[39m module\u001b[38;5;241m.\u001b[39mcpu()\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Cannot allocate Linear(in_features=500000, out_features=4, bias=True) to cuda"
     ]
    }
   ],
   "source": [
    "model_0.eval()\n",
    "with torch.inference_mode():\n",
    "  y_pred = model_0(X)\n",
    "\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wed Jul 24 17:14:25 2024       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 525.147.05   Driver Version: 525.147.05   CUDA Version: 12.0     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:24:00.0 Off |                  N/A |\n",
      "| 30%   42C    P2   112W / 350W |   1253MiB / 24576MiB |      1%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter 'linear_layer_1.weight' is on device: cuda:0\n",
      "Parameter 'linear_layer_1.bias' is on device: cuda:0\n",
      "Parameter 'linear_layer_2.weight' is on device: cuda:0\n",
      "Parameter 'linear_layer_2.bias' is on device: cuda:0\n",
      "Parameter 'linear_layer_3.weight' is on device: cuda:0\n",
      "Parameter 'linear_layer_3.bias' is on device: cuda:0\n",
      "Parameter 'linear_layer_4.weight' is on device: cuda:0\n",
      "Parameter 'linear_layer_4.bias' is on device: cuda:0\n",
      "Parameter 'linear_layer_5.weight' is on device: cuda:0\n",
      "Parameter 'linear_layer_5.bias' is on device: cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Verify device allocation for each layer\n",
    "for name, layer in model_0.named_parameters():\n",
    "    print(f\"Parameter '{name}' is on device: {layer.device}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
